{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mothur Markdown (and other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Markdown file lists the workflow used to recreate the work done by Shengyong Mao et al in\n",
    "### Characterising the bacterial microbiota across the gastrointestinal tracts of dairy cattle: membership and potential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SRA download\n",
    "\n",
    "Initially the files needed to be downloaded from the SRA database provided by NCBI. This was done using the SRA toolkit within the shell.\n",
    "\n",
    "To locate the file and download it using the command\n",
    "#you will need to navigate to the folder you have the SRA toolkit downloaded to and run these commands there\n",
    "\n",
    "`$prefetch -v SRR1778214` \n",
    "\n",
    "#this command will create a file called ncbi w/in the users directory.\n",
    "\n",
    "to split the sequences into forward and reverse reads, navigate to your working directory and use the command\n",
    "\n",
    "`$fastq-dump --split-files (/home/[USER]/ncbi/public/sra/SRR1778214.sra)`\n",
    "\n",
    "#alternatively, you can use the -outdir (/output directory) argument to direct the output to your designated directory. Using this will save you the hassle of navigating to the output directory\n",
    "#the --split-files argument should split the file into forward and reverse reads, however, ours did not. We tried the argument --split-3 which is meant to split and tailor the reads to paired and unpaired, but only one file was output, suggesting only forward reads were present or detectable.\n",
    "#This was rather frustrating due to fact they claimed to use \"paired end\" sequencing, which would make quality control and sequence analysis more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENA download\n",
    "\n",
    "We could not figure out if the file had failed to organize the sequences in a way that the SRA toolkit could recognize forward and reverse, or if it simply did not have reverse included.\n",
    "\n",
    "We also downloaded the file using the European Nucleotide Database provided program, which did not solve our issue.\n",
    "\n",
    "Conclusion: forward sequences only were uploaded for this particular experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mothur\n",
    "\n",
    "To download the latest version of mothur (this analysis used 1.39.0 and is provided within the repositroy). please visit (https://www.mothur.org/wiki/Download_mothur) if you would like to download any other files that aren't mentioned in this analysis.\n",
    "\n",
    "This \"download\" page gives you access to all files used in this analysis besides the sequences themselves. We used the latest Silva reference files (silva.bacterial.fasta and silva.gold.align), RDP files (trainset_16), and the 13.5 Greengenes reference file. all of these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by transferring all reference files and the program to a directory containing your fastq file(s)\n",
    " \n",
    "run the program. It should open it's own shell with the prompt \">\"\n",
    "\n",
    "__1.__\n",
    "\n",
    "initially we needed to convert the fastq file into the fasta file (containing just sequences w/out quality)\n",
    "\n",
    "`>fastq.info(fastq=SRR1778214.fastq)`\n",
    "\n",
    "should return:\n",
    "\n",
    "`Output File Names: \n",
    "SRR1778214.fasta\n",
    "SRR1778214.qual`\n",
    "\n",
    "then check the sequences\n",
    "\n",
    "`>summary.seqs(fasta=SRR1778214.fasta)`\n",
    "\n",
    "output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using 1 processors.\n",
    "\n",
    "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
    "Minimum:\t1\t68\t68\t0\t3\t1\n",
    "2.5%-tile:\t1\t425\t425\t0\t4\t1259\n",
    "25%-tile:\t1\t426\t426\t0\t4\t12581\n",
    "Median: \t1\t441\t441\t0\t5\t25162\n",
    "75%-tile:\t1\t446\t446\t0\t5\t37742\n",
    "97.5%-tile:\t1\t451\t451\t0\t6\t49064\n",
    "Maximum:\t1\t456\t456\t6\t9\t50322\n",
    "Mean:\t1\t437.052\t437.052\t0.000616033\t4.84776\n",
    "#of Seqs:\t50322\n",
    "\n",
    "Output File Names: \n",
    "SRR1778214.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.__\n",
    "\n",
    "From the two files generated, command mothur to trim the sequences with lower that 20 for PHRED qulaity score over a sliding window of 10b. \n",
    "\n",
    "`>trim.seqs(fasta=SRR.fasta, qfile=SRR.qual, qwindowsize=10, qwindowaverage=20, minlength=426, maxhomop=8, maxambig=0)`\n",
    "#arguments qfile=specifies the corresponding quality file, qwindowsize set the rolling window size, qwindowaverage sets the quality score cutoff\n",
    "#notice, I manually changed the name of my fasta file to SRR.fasta. Shorter names are always better if you can maintain important info.\n",
    "\n",
    "output:\n",
    "\n",
    "`Output File Names: \n",
    "SRR.trim.fasta\n",
    "SRR.scrap.fasta\n",
    "SRR.trim.qual\n",
    "SRR.scrap.qual`\n",
    "\n",
    "Then check the sequences again\n",
    "\n",
    "`>summary.seqs(fasta=SRR.trim.fasta)`\n",
    "\n",
    "output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using 1 processors.\n",
    "\n",
    "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
    "Minimum:\t1\t68\t68\t0\t3\t1\n",
    "2.5%-tile:\t1\t425\t425\t0\t4\t1259\n",
    "25%-tile:\t1\t426\t426\t0\t4\t12581\n",
    "Median: \t1\t441\t441\t0\t5\t25162\n",
    "75%-tile:\t1\t446\t446\t0\t5\t37742\n",
    "97.5%-tile:\t1\t451\t451\t0\t6\t49064\n",
    "Maximum:\t1\t456\t456\t6\t9\t50322\n",
    "Mean:\t1\t437.006\t437.006\t0.000616033\t4.84774\n",
    "# of Seqs:\t50322\n",
    "\n",
    "Output File Names: \n",
    "SRR.trim.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that nothing changed here. Why is that? We think it may be that they did some quality control before they uploaded the sequences. Not 100% sure here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.__\n",
    "\n",
    "Now we do something that is normally usd to handle many sequences from different samples with different barcodes. We will condense these different identifiers into \"groups\". Even though we only have one sample, this command is needed to make a file used downstream. This command is, however, very useful if handling large sample amounts.\n",
    "\n",
    "`make.group(fasta=SRR.trim.fasta, groups=Rumen_wall)`\n",
    "#name the group whatever you would like\n",
    "\n",
    "output:\n",
    "\n",
    "`Output File Names: SRR.trim.groups`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.__\n",
    "\n",
    "We will now condence the total number of sequences into unique sequences. This means we will take all of the sequences that are >96% similar select one representative sequence for the entire group. All the sequences will be removed except for the representative sequence, and a count will be included in the \"names\" file as meta data, keeping track of condensed sequences. The command is:\n",
    "\n",
    "`>unique.seqs(fasta=SRR.trim.fasta)`\n",
    "\n",
    "output:\n",
    "\n",
    "`Output File Names: \n",
    "SRR.trim.names\n",
    "SRR.trim.unique.fasta`\n",
    "\n",
    "#the names file is produced here, and is mentioned above. the fasta file will continue down the pipe.\n",
    "\n",
    "Make sure to check the sequences now:\n",
    "\n",
    "`>summary.seqs(fasta=SRR.trim.unique.fasta)`\n",
    "\n",
    "output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using 4 processors.\n",
    "[WARNING]: This command can take a namefile and you did not provide one. The current namefile is SRR.trim.names which seems to match SRR.trim.unique.fasta.\n",
    "\n",
    "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
    "Minimum:\t1\t68\t68\t0\t3\t1\n",
    "2.5%-tile:\t1\t424\t424\t0\t4\t711\n",
    "25%-tile:\t1\t427\t427\t0\t4\t7106\n",
    "Median: \t1\t441\t441\t0\t5\t14211\n",
    "75%-tile:\t1\t445\t445\t0\t5\t21316\n",
    "97.5%-tile:\t1\t451\t451\t0\t6\t27710\n",
    "Maximum:\t1\t456\t456\t6\t9\t28420\n",
    "Mean:\t1\t437.135\t437.135\t0.00109078\t4.84152\n",
    "# of Seqs:\t28420\n",
    "\n",
    "Output File Names: \n",
    "SRR.trim.unique.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#you can include the name file like this, to remove the warning:\n",
    "\n",
    "`>summary.seqs(fasta=SRR.trim.unique.fasta, name=SRR.trim.names)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#notice that the number of sequences were condensed by half this means that there are 28,420 sequences that were different in this file (<96%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.__\n",
    "\n",
    "here we will make a file fore downstream use, once again. This file is called the count_table file, and will include information about the sequences such as the number of sequences condensed per OTU (uniqe.seqs). It will also be able to keep groups distinct, if we had any.\n",
    "\n",
    "`count.seqs(name=SRR.trim.names, group=SRR.trim.groups)`\n",
    "\n",
    "output:\n",
    "\n",
    "`Using 4 processors.\n",
    "It took 0 secs to create a table for 43974 sequences.\n",
    "\n",
    "\n",
    "Total number of sequences: 43974\n",
    "\n",
    "Output File Names:\n",
    "SRR.trim.count_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.__\n",
    "\n",
    "Although you can use the pcr.seq command here to shrink the alignment reference file to a more manageable size, for our final analysis we skipped this. the silva.bacterial.fasta file is ok.\n",
    "\n",
    "here is the command anyways... if you wanted to include it:\n",
    "\n",
    "`pcr.seqs(fasta=silva.bacterial.fasta, oligos=MyOligos.oligos)`\n",
    "#the fasta provided is the silva reference file and the oligos file is a file containing my forward primer sequence and reverse primer sequence. these will be mapped to the reference.\n",
    "\n",
    "We moved straight to aligning the sequences. for this, we used the command:\n",
    "\n",
    "`align.seqs(fasta=SRR.trim.unique.fasta, reference=silva.bacteria.fasta, processors=4)`\n",
    "#here we are providing the fasta file to align, providing the reference database (in the final analysis we are using the silva in one final attempt to get it to accurately be classified), and using all 4 of my local processors. check the number of processors your machine has before running this command.\n",
    "\n",
    "output:\n",
    "\n",
    "`Output File Names:\n",
    "SRR.trim.unique.align\n",
    "SRR.trim.unique.align.report\n",
    "SRR.trim.unique.flip.accnos`\n",
    "\n",
    "and check it:\n",
    "\n",
    "`summary.seqs(fasta=SRR.trim.unique.align, name=SRR.trim.unique.fasta)`\n",
    "\n",
    "output:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                Start   End     NBases  Ambigs  Polymer NumSeqs\n",
    "Minimum:        0       0       0       0       1       1\n",
    "2.5%-tile:      6426    25318   426     0       4       608\n",
    "25%-tile:       6426    25318   428     0       4       6077\n",
    "Median:         6426    25318   443     0       5       12153\n",
    "75%-tile:       6426    25318   446     0       5       18229\n",
    "97.5%-tile:     6426    25318   451     0       6       23697\n",
    "Maximum:        43115   43116   455     0       8       24304\n",
    "Mean:   6450.8  25319.1 439.455 0       4.81468\n",
    "#of Seqs:      24304\n",
    "\n",
    "Output File Names:\n",
    "SRR.trim.unique.summary\n",
    "\n",
    "It took 22 secs to summarize 24304 sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.__\n",
    "\n",
    "Now we will check the fasta file for chimeric sequences and remove them. Luckily, mothur offers the uchime package and is compatible with a silva reference file strictly for removing chimeras. We used this code for processing chimeras:\n",
    "\n",
    "`chimera.uchime(fasta=SRR.trim.unique.align, dereplicate=t, reference=silva.gold.align, processors=4)`\n",
    "#this is supplying the fasta file to be checked, and what reference should be used, in this case the silva.gold.align file.\n",
    "#dereplicate just says that if one group finds the sequences to be chimeric, the rest will too. this saves time and is suggested to be included, but in this case, we are only using one group and therefore it really doesn't do anything.\n",
    "\n",
    "#additionally, this command makes a bunch of crazy stuff happen on the screen. we can't tell you why, but it is cleared up upon completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8.__ \n",
    "\n",
    "Lets get those chimeras out of our fasta file!\n",
    "\n",
    "`remove.seqs(fasta=SRR.trim.unique.align, accnos=SRR.trim.unique.ref.uchime.accnos)`\n",
    "\n",
    "this command finds the flagged chimeric sequences in the original fasta and removes them.\n",
    "\n",
    "then make sure you check that new fasta:\n",
    "\n",
    "`summary.seqs(fasta=SRR.trim.unique.pick.align)`\n",
    "\n",
    "output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using 4 processors.\n",
    "[WARNING]: This command can take a namefile and you did not provide one. The current namefile is SRR.trim.names which seems to match SRR.trim.unique.pick.align.\n",
    "\n",
    "                Start   End     NBases  Ambigs  Polymer NumSeqs\n",
    "Minimum:        0       0       0       0       1       1\n",
    "2.5%-tile:      6426    25318   426     0       4       591\n",
    "25%-tile:       6426    25318   428     0       4       5908\n",
    "Median:         6426    25318   443     0       5       11815\n",
    "75%-tile:       6426    25318   446     0       5       17722\n",
    "97.5%-tile:     6426    25318   451     0       6       23039\n",
    "Maximum:        43115   43116   455     0       8       23629\n",
    "Mean:   6447.74 25319.2 439.491 0       4.81273\n",
    "# of Seqs:      23629\n",
    "\n",
    "Output File Names:\n",
    "SRR.trim.unique.pick.summary\n",
    "\n",
    "It took 21 secs to summarize 23629 sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9.__\n",
    "\n",
    "Finally, we make it to the classification step. Unfortunately, we could not make it work... However, we think it can, provided the correct input. We feel that this software is truly very powerful, being user friendly and compatible with both mac and windows.  \n",
    "\n",
    "Anyway, onto the code we used for this analysis:\n",
    "\n",
    "`classify.seqs(fasta=SRR.trim.unique.pick.align, count=SRR.trim.count_table, reference=trainset16_022016.rdp.fasta, taxonomy=trainset16_022016.rdp.tax, processors=4)`\n",
    "\n",
    "#this command uses the fasta file you want to classify, the count table we generated earlier, the reference form the RDP database, the taxonomy file from the RDP that contains names for each reference, and the number of processors going.\n",
    "#it is important to note here that the RDP files only classify to genus level. not that it mattered.\n",
    "\n",
    "output:\n",
    "\n",
    "`[WARNING]: mothur reversed some your sequences for a better classification.  If you would like to take a closer look, please check SRR.trim.unique.pick.rdp.wang.flip.accnos for the list of the sequences.\n",
    "\n",
    "It took 313 secs to classify 23629 sequences.\n",
    "\n",
    "\n",
    "It took 1 secs to create the summary file for 23629 sequences.\n",
    "\n",
    "\n",
    "Output File Names:\n",
    "SRR.trim.unique.pick.rdp.wang.taxonomy\n",
    "\n",
    "SRR.trim.unique.pick.rdp.wang.tax.summary\n",
    "\n",
    "SRR.trim.unique.pick.rdp.wang.flip.accnos`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you open the \"SRR.trim.unique.pick.rdp.wang.taxonomy\" file with Excel, you will see what we mean when we say it did not work.\n",
    "\n",
    "we are not sure if it is versioning, poor upstream analysis or if it was bad data, but we did learn some about mothur and feel that, in the right hands, it could be very powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://www.mothur.org/wiki/Main_Page\n",
    "\n",
    "https://www.mothur.org/wiki/RDP_reference_files\n",
    "\n",
    "https://www.mothur.org/wiki/Silva_reference_files\n",
    "\n",
    "https://www.mothur.org/wiki/Greengenes-formatted_databases\n",
    "\n",
    "http://blog.mothur.org/2016/07/07/Customization-for-your-region/\n",
    "\n",
    "https://www.protocols.io/view/week-8-classifying-taxonomy-of-short-reads-with-mo-g7tbznn\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4630781/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
